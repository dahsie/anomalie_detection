{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c662b53e-e0e1-4f69-adb8-a159f7d83b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LinearLR,ReduceLROnPlateau  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer, roc_auc_score,confusion_matrix\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import RocCurveDisplay,roc_curve\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71c776-8a84-4d83-aec0-84550aea4192",
   "metadata": {},
   "source": [
    "### Dataset customisée par entraîner un modèle d'auto encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aaeb23-22a9-4f8a-b657-7914e1f2e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_Dataset(Dataset):\n",
    "\n",
    "    \"\"\"Cette classe à été créée afin de créer facilement un DataLoader de pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "\n",
    "        super(AE_Dataset,self).__init__()\n",
    "        \n",
    "        #La target d'un auto encoder est lui le donnée initial car n'oublions pas que l'objectif \n",
    "        #de l'auto encodeur c'est de reconstituer la donnée initiale\n",
    "\n",
    "        self.data = np.copy(data)\n",
    "        self.targets = np.copy(data)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index],dtype=torch.float32), torch.tensor(self.targets[index],dtype=torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc24e95-4789-45f1-b4a3-17e92c6e74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusin_matrix_display(ytest,ypred):\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix(ytest,ypred)).plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def precision_recall_display(ytest, ypred):\n",
    "    precision, recall, _ = precision_recall_curve(ytest, ypred)\n",
    "    PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "    plt.title(\"Precision_Recall_Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def roc_curve_disolay(ytest, ypred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(ytest, ypred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "    print(f\"roc_auc {roc_auc}\")\n",
    "    display.plot()\n",
    "    plt.title(\"ROC_Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def scores(ytest,ypred):\n",
    "    df=pd.DataFrame({\"f1_score\" : [f1_score(ytest,ypred)],\n",
    "                     \"accuracy_score\" : [accuracy_score(ytest,ypred)],\n",
    "                     \"precision_score\" : [precision_score(ytest,ypred)],\n",
    "                     \"recall_score\" : [recall_score(ytest,ypred)]\n",
    "    })\n",
    "    print(df)\n",
    "    return df\n",
    " \n",
    "def evaluation(ytrue,ypred):\n",
    "    confusin_matrix_display(ytrue,ypred)\n",
    "    precision_recall_display(ytrue, ypred)\n",
    "    roc_curve_disolay(ytrue, ypred)\n",
    "    scores(ytest,ypred)\n",
    "\n",
    "def transform_target(target):\n",
    "    \"\"\"Args:\n",
    "        target: np.ndarry or pd.DataFrame\n",
    "    \"\"\"\n",
    "    n=target.shape[0]\n",
    "    if isinstance(target,np.ndarray):\n",
    "        target[target==1]=-1\n",
    "        target[target==0]=1\n",
    "        \n",
    "    elif isinstance(target,pd.DataFrame) or isinstance(target,pd.Series):\n",
    "        target[target==1]=-1\n",
    "        target[target==0]=1\n",
    "    return target\n",
    "\n",
    "\n",
    "def inverse_transform_target(target):\n",
    "    n=target.shape[0]\n",
    "    if isinstance(target,pd.DataFrame) or isinstance(target,pd.Series):\n",
    "        target[target==1]=0\n",
    "        target[target==-1]=1\n",
    "        \n",
    "    elif isinstance(target,np.ndarray): \n",
    "        target[target==1]=0\n",
    "        target[target==-1]=1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c9f6e-2d7c-4386-8706-042b5acf2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "\n",
    "    def __init__(self, file_path, scaler,test_size=0.15, random_state=42):\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        self.data = None\n",
    "        self.target = None\n",
    "\n",
    "        # Chargement et prétraitement de la donnée\n",
    "        self._load_data()\n",
    "        self._preprocess_data()\n",
    "        self._split_data()\n",
    "\n",
    "        self.xtrain = self._xtrain # ce sont uniquement les données normales quui seront entraînées avec l'auto encoder\n",
    "        self.ytrain = self.xtrain # La target d'un auto encoder est lui le donnée initial car n'oublions pas que l'objectif \n",
    "                                # de l'auto encodeur c'est de reconstituer la donnée initiale\n",
    "\n",
    "        self.xtest = pd.concat([self._xtest, self.data[self.target==1]], axis=0)\n",
    "        self.ytest = pd.concat([self._ytest, self.target[self.target==1]], axis=0)\n",
    "\n",
    "        self.xtest.reset_index(drop=True, inplace=True)\n",
    "        self.ytest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Note : ytrain utilisé comme target lors de l'entraînement n'est le même format que ytest utilisé lors de la prédiction\n",
    "        # ytrain =xtrain alors que ytest =target (labels)(0 pour donnée normale et 1 pour donnée anormale)\n",
    "   \n",
    "        \n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "        \n",
    "    def _load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        \n",
    "        self.data.drop_duplicates(keep=\"last\", inplace=True)\n",
    "        self.target = self.data[\"Class\"]\n",
    "        self.data.drop([\"Class\", \"Time\"], axis=1,inplace=True)\n",
    "        \n",
    "\n",
    "    def _preprocess_data(self):\n",
    "\n",
    "        # Avec un modèle de réseaux de neurones profond, l'on n'est pas obligé de pré-traiter la donnée\n",
    "        if self.scaler is not None:\n",
    "            \n",
    "            data_scaled_array = self.scaler.fit_transform(self.data)\n",
    "            \n",
    "            self.data = pd.DataFrame(data_scaled_array, columns=self.data.columns,index=self.data.index)\n",
    "            \n",
    "\n",
    "    def _split_data(self):\n",
    "        \n",
    "        self._xtrain, self._xtest, self._ytrain, self._ytest = train_test_split(\n",
    "            self.data, self.target, test_size=self.test_size, random_state=self.random_state, shuffle=True)\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.xtrain, self.ytrain\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.xtest, self.ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55e561-7f32-41a5-88f2-1d008b9f5ddf",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cbce9-1d22-4703-84cd-5db8817a9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size: int, latent_size: int,dropout: float) -> None:\n",
    "\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=input_size,out_features=25)\n",
    "        self.residual1=nn.Linear(in_features=input_size,out_features=20)\n",
    "        self.layer_norm1=nn.LayerNorm(25)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "        self.fc2=nn.Linear(in_features=25,out_features=20)\n",
    "        self.layer_norm2=nn.LayerNorm(20)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.residual2=nn.Linear(in_features=20,out_features=latent_size)\n",
    "        self.fc3=nn.Linear(in_features=20,out_features=15)\n",
    "        self.layer_norm3=nn.LayerNorm(15)\n",
    "        self.relu3=nn.ReLU()\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "        self.fc4=nn.Linear(in_features=15,out_features=latent_size)\n",
    "        \n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        \n",
    "        x=self.relu1(self.layer_norm1(self.fc1(inputs)))\n",
    "        residual1=self.residual1(inputs)\n",
    "        x=self.dropout1(x)\n",
    "        x= self.relu2(self.layer_norm2(self.fc2(x)+residual1))\n",
    "        residual2=self.residual2(x)\n",
    "        x= self.relu3(self.layer_norm3(self.fc3(x)))\n",
    "        x=self.dropout2(x)\n",
    "        x=self=self.fc4(x) +residual2\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f9ea2-841a-432e-b478-48cadadaabf0",
   "metadata": {},
   "source": [
    "## Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab96b06-48b5-4d07-bd10-697c99e889f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,latent_size: int, output_size: int,dropout: float) -> None:\n",
    "\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=latent_size,out_features=15)\n",
    "        self.residual1=nn.Linear(in_features=latent_size,out_features=20)\n",
    "        self.relu1=nn.LeakyReLU(0.1)\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "        self.fc2=nn.Linear(in_features=15,out_features=20)\n",
    "        self.layer_norm3=nn.LayerNorm(20)\n",
    "        self.relu2=nn.LeakyReLU(0.1)\n",
    "        self.residual2=nn.Linear(in_features=20,out_features=29)\n",
    "        self.fc3=nn.Linear(in_features=20,out_features=25)\n",
    "        self.layer_norm4=nn.LayerNorm(25)\n",
    "        self.relu3=nn.LeakyReLU(0.1)\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc4=nn.Linear(in_features=25,out_features=output_size)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        x=self.relu1(self.fc1(inputs))\n",
    "        residual1=self.residual1(inputs)\n",
    "        x=self.dropout1(x)\n",
    "        x= self.relu2(self.layer_norm3(self.fc2(x) +residual1))\n",
    "        residual2=self.residual2(x)\n",
    "        x= self.relu3(self.layer_norm4(self.fc3(x)))\n",
    "        x=self.dropout2(x)\n",
    "        x=self=self.fc4(x) +residual2\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c14f8f-0a44-4d7a-8097-e128fc3bfe3f",
   "metadata": {},
   "source": [
    "## AutoEncoder/ Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353d927-e332-4123-94c2-dda7fcbfc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size: int,latent_size: int,output_size: int,dropout: float) -> None:\n",
    "\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        \n",
    "        self.encoder=Encoder(input_size=input_size, latent_size = latent_size,dropout=dropout)\n",
    "        self.latent=None\n",
    "        self.decoder=Decoder(latent_size =latent_size, output_size=output_size,dropout=dropout)\n",
    "        \n",
    "\n",
    "        self.train_curve=[]\n",
    "        self.val_curve=[]\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        x=self.encoder(inputs)\n",
    "        self.latent=x\n",
    "        x=self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68428fd3-1c6f-452a-b54e-7aa3cbcade45",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45bc5e-0752-47db-a2df-96e3a0d1fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size: int,dropout: float) -> None:\n",
    "\n",
    "        super(Discriminator,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=input_size,out_features=25)\n",
    "        self.residual1=nn.Linear(in_features=input_size,out_features=20)\n",
    "        self.layer_norm1=nn.LayerNorm(25)\n",
    "        self.relu1=nn.LeakyReLU(0.2)\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "        self.fc2=nn.Linear(in_features=25,out_features=20)\n",
    "        self.layer_norm2=nn.LayerNorm(20)\n",
    "        self.relu2=nn.LeakyReLU(0.2)\n",
    "        self.residual2=nn.Linear(in_features=20,out_features=5)\n",
    "        self.fc3=nn.Linear(in_features=20,out_features=15)\n",
    "        self.layer_norm3=nn.LayerNorm(15)\n",
    "        self.relu3=nn.LeakyReLU(0.2)\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "        self.fc4=nn.Linear(in_features=15,out_features=5)\n",
    "        self.fc5=nn.Linear(in_features=5,out_features=1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        \n",
    "        x=self.relu1(self.layer_norm1(self.fc1(inputs)))\n",
    "        residual1=self.residual1(inputs)\n",
    "        x=self.dropout1(x)\n",
    "        x= self.relu2(self.layer_norm2(self.fc2(x)+residual1))\n",
    "        residual2=self.residual2(x)\n",
    "        x= self.relu3(self.layer_norm3(self.fc3(x)))\n",
    "        x=self.dropout2(x)\n",
    "        out=self.fc4(x) +residual2\n",
    "        x=self.fc5(out)\n",
    "        x=self.sigmoid(x)\n",
    "\n",
    "        return out,x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1821d07-87dc-4d5a-8683-80858eb0f9e0",
   "metadata": {},
   "source": [
    "## GANomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906a9f8-6db0-4df1-b7b1-76e615a5a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANomaly(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size: int,latent_size: int,contamination: float=0.02,dropout: float=0.1,device: str=\"cuda\") -> None:\n",
    "\n",
    "        super(GANomaly,self).__init__()\n",
    "        \n",
    "        self.generator = AutoEncoder(input_size=input_size,latent_size=latent_size,output_size=input_size,dropout=dropout) \n",
    "        self.discriminator=Discriminator(input_size=input_size,dropout=dropout)\n",
    "        self.encoder=Encoder(input_size=input_size, latent_size = latent_size,dropout=dropout)\n",
    "        self.latent_size=latent_size\n",
    "        self.offset=contamination\n",
    "        self.epoch=1\n",
    "        \n",
    "        self.device= torch.device(\"cuda\") if device==\"cuda\" else torch.device(\"cpu\")\n",
    "        self.is_fitted=False\n",
    "\n",
    "        self.generator_loss=[]\n",
    "        self.discriminator_loss=[]\n",
    "        self.encoder_loss=[]\n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def train_one_epoch(self,train_loader:DataLoader,Gen_optimizer,Discr_optimizer,Enc_optimizer,loss_fn,recons_loss,Gen_scheduler,En_scheduler):\n",
    "        \n",
    "        \"\"\"The function is use to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer      : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function  : The loss function to use during the model training. CrossEntropyLoss and MSELoss are very often use for respectively\n",
    "                                classification and regression problem\n",
    "                train_loader   : A torch DataLoader which is use to iterate through the data during train step\n",
    "\n",
    "            Return : The last loss value on the train_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        last_disc_loss=0 # Computing loss per batch\n",
    "        last_gen_loss=0\n",
    "        last_enc_loss=0\n",
    "        running_enc_loss=0\n",
    "        running_discr_loss=0\n",
    "        running_gen_loss=0\n",
    "        \n",
    "        \n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "\n",
    "\n",
    "            ##--------------------------------------Entrainement du Discriminateur -------------------------------------------------------------\n",
    "            \n",
    "            Discr_optimizer.zero_grad()\n",
    "            #real data\n",
    "            real_data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "\n",
    "            #Fake data\n",
    "            fake_data=self.generator.forward(real_data)\n",
    "\n",
    "            batch_size=real_data.shape[0]\n",
    "\n",
    "            #Forwards pass and loss computing for real data\n",
    "            real_outputs,_=self.discriminator.forward(real_data)\n",
    "            fake_output,_=self.discriminator.forward(fake_data)\n",
    "            disc_loss=loss_fn(real_outputs.to(self.device),fake_output) # Computing the loss\n",
    "            disc_loss.backward() # Computing the gradient\n",
    "\n",
    "            running_discr_loss+=disc_loss.item()\n",
    "\n",
    "            Discr_optimizer.step()\n",
    "            \n",
    "\n",
    "            ##--------------------------------------Entrainement du L'encoder -------------------------------------------------------------\n",
    "\n",
    "            Enc_optimizer.zero_grad()\n",
    "            \n",
    "            fake_data1=self.generator.forward(real_data)\n",
    "            z_real1=self.generator.latent\n",
    "            z_fake1=self.encoder(fake_data1)\n",
    "\n",
    "            assert z_real1.shape==z_fake1.shape, \"z_real and z_fake should have the same shape\"\n",
    "            \n",
    "            enc_loss=loss_fn(z_real1,z_fake1)\n",
    "            enc_loss.backward()\n",
    "\n",
    "            running_enc_loss+=enc_loss.item()\n",
    "\n",
    "            Enc_optimizer.step()\n",
    "            \n",
    "\n",
    "            ##--------------------------------------Entrainement du Generateur ------------------------------------------------------------\n",
    "            # Discr_optimizer.zero_grad()\n",
    "            # Enc_optimizer.zero_grad()\n",
    "            Gen_optimizer.zero_grad()\n",
    "\n",
    "            fake_data2=self.generator.forward(real_data)\n",
    "            z_real2=self.generator.latent\n",
    "            z_fake2=self.encoder(fake_data2)\n",
    "            real_outputs2,_=self.discriminator.forward(real_data)\n",
    "            fake_output2,_=self.discriminator.forward(fake_data2)\n",
    "            gen_loss= 1*loss_fn(z_real2,z_fake2) + 3*loss_fn(real_outputs2.to(self.device),fake_output2) + 50*recons_loss(real_data,fake_data2) # selon le document officieel\n",
    "\n",
    "            gen_loss.backward()\n",
    "\n",
    "            running_gen_loss+=gen_loss.item()\n",
    "\n",
    "            Gen_optimizer.step()\n",
    "            # Gen_optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "\n",
    "            if batch_index%100==99:\n",
    "                last_disc_loss=running_discr_loss/100 # Computing loss per batch\n",
    "                last_gen_loss=running_gen_loss/100\n",
    "                last_enc_loss=running_enc_loss/100\n",
    "                \n",
    "                running_discr_loss=0\n",
    "                running_gen_loss=0\n",
    "                running_enc_loss=0\n",
    "                # print(\"batch {} loss {}\".format(batch_index+1,last_loss))\n",
    "        # \n",
    "        if self.epoch %10==0:\n",
    "            # before_lr = Gen_optimizer.param_groups[0][\"lr\"]\n",
    "            Gen_scheduler.step()\n",
    "            En_scheduler.step()\n",
    "            # after_lr = Gen_optimizer.param_groups[0][\"lr\"]\n",
    "            # print(\"SGD lr %.8f -> %.8f\" % (before_lr, after_lr))\n",
    "        return last_gen_loss,last_disc_loss,last_enc_loss\n",
    "    \n",
    "    def predict(self, dataloader: DataLoader, contamination: float=0.02, criterion=nn.L1Loss(reduction='none')):\n",
    "        assert self.is_fitted==True, \"Le modèle n'est pas encore entraîné donc ne peut pas faire de prédiction\"\n",
    "\n",
    "        self.offset=contamination\n",
    "        return self.decision_function(dataloader,criterion)\n",
    "\n",
    "\n",
    "    def decision_function(self,dataloader: DataLoader,loss_fn):\n",
    "        \n",
    "        dataframe=self.compute_score_sample(dataloader,loss_fn)\n",
    "        dataframe.loc[-dataframe[\"score_sample\"] < (-dataframe[\"score_sample\"]).quantile(self.offset), \"label\"] = -1\n",
    "        \n",
    "        return dataframe['label']\n",
    "\n",
    "    def score_sample(self,dataloader: DataLoader,loss_fn):\n",
    "        \n",
    "        dataframe=self.compute_score_sample(self,dataloader,loss_fn)\n",
    "\n",
    "        return dataframe[\"score_sample\"]\n",
    "\n",
    "   \n",
    "    def compute_score_sample(self,dataloader: DataLoader,loss_fn):\n",
    "        \n",
    "        reconstition_errors=[]\n",
    "        with torch.no_grad() :\n",
    "            for batch in dataloader:\n",
    "    \n",
    "                data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "                outputs=self.generator.forward(data) \n",
    "\n",
    "                z_g_e=self.generator.latent\n",
    "                z_enc=self.encoder(data)\n",
    "\n",
    "                \n",
    "                loss=loss_fn(z_enc,z_g_e) # Computing the loss\n",
    "\n",
    "                for item in loss:\n",
    "                    reconstition_errors.append(torch.sum(item).item())\n",
    "\n",
    "        scaler=MinMaxScaler()\n",
    "        recons_scaled=scaler.fit_transform(np.array(reconstition_errors).reshape(-1,1))\n",
    "        reconstition_errors=list(recons_scaled.reshape(-1))\n",
    "        dataframe=pd.DataFrame({\n",
    "            \"score_sample\" : reconstition_errors,\n",
    "            \"label\" :[1 for _ in range(len(reconstition_errors))]\n",
    "        })\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    \n",
    "    def getEpoch(self):\n",
    "        return self.epoch\n",
    "        \n",
    "    def fit(self,train_loader,Gen_optimizer,Disc_optimizer,Enc_optimizer,Gen_scheduler,En_scheduler,loss_fn=nn.MSELoss,recons_loss=nn.L1Loss,epochs=10,device=\"cuda\"):\n",
    "        self.epoch=epochs\n",
    "        \"\"\"The function is used to train the model through many epochs\n",
    "            Args: \n",
    "                optimizer             : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                train_loader          : A torch DataLoader which is use to iterate through the data during train step\n",
    "                loss_function         : The loss function to use during the model validation. Same as the training one\n",
    "                validataion_loader    : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : No return\n",
    "                \n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "        best_avg_vloss=1_000_000_000\n",
    "        for epoch in range(epochs):\n",
    "        \n",
    "            print(\"EPOCHS : {}\".format(epoch +1))\n",
    "            self.train(True) # Activate training mode\n",
    "            avg_loss_gen,avg_loss_disc,avg_loss_enc=self.train_one_epoch(train_loader,Gen_optimizer,Disc_optimizer,Enc_optimizer,loss_fn,recons_loss,Gen_scheduler,En_scheduler) # training the model on an epoch\n",
    "            \n",
    "            print(f\"Gen Loss {avg_loss_gen} vs Disc Loss  {avg_loss_disc} vs Enc Loss {avg_loss_enc}\")\n",
    "            self.generator_loss.append(avg_loss_gen)\n",
    "            self.discriminator_loss.append(avg_loss_disc)\n",
    "            self.encoder_loss.append(avg_loss_enc)\n",
    "\n",
    "            if avg_loss_disc < best_avg_vloss :\n",
    "                best_avg_vloss=avg_loss_disc\n",
    "                model_path='/home/dah/anomalie_detection/anomalie_detection/models/model_GANomaly_{}'.format(datetime.now().strftime('%Y%m%d_%H%')) \n",
    "                torch.save(self.cpu().state_dict(),model_path)\n",
    "                self.to(self.device)\n",
    "                self.to(self.device)\n",
    "            self.epoch+=1\n",
    "        self.is_fitted=True\n",
    "\n",
    "        return {\n",
    "            \"generator_loss\":self.generator_loss,\n",
    "            \"discriminator_loss\":self.discriminator_loss,\n",
    "            \"encoder_loss\":self.encoder_loss\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c20c6c-762f-4ce2-804e-fd2bf8ebba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprodductibilité\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Creation Dataset \n",
    "dataset = CustomDataset(file_path=\"/home/dah/anomalie_detection/anomalie_detection/data/creditcard.csv\",scaler=MinMaxScaler())\n",
    "\n",
    "xtrain, ytrain = dataset.get_train_data()\n",
    "xtest, ytest = dataset.get_test_data()\n",
    "\n",
    "\n",
    "#Creation de DataLoader pytorch (Just un objet qui permet d'itérer plus faciler sur l'entièreté de la dataset)\n",
    "pytorch_train_dataset = AE_Dataset(xtrain)\n",
    "pytorch_test_dataset = AE_Dataset(xtest)\n",
    "train_loader=DataLoader(dataset=pytorch_train_dataset,batch_size=100)\n",
    "test_loader=DataLoader(dataset=pytorch_test_dataset,batch_size=10)\n",
    "\n",
    "#Creation de modeèle\n",
    "model=GANomaly(input_size=29,latent_size=10,contamination=0.02,dropout=0.2,device=\"cpu\")\n",
    "\n",
    "#Optimiseur du generator\n",
    "Gen_optimizer=optim.RMSprop(lr=1e-3, params=model.generator.parameters()) # Optimiseur\n",
    "\n",
    "Gen_scheduler = torch.optim.lr_scheduler.ExponentialLR(Gen_optimizer, gamma=0.1)\n",
    "\n",
    "#Optimiseur du generator\n",
    "Disc_optimizer=optim.SGD(lr=1e-3, params=model.discriminator.parameters(),momentum=0.9) # Optimiseur\n",
    "\n",
    "#Optimiseur du generator de l'encoder\n",
    "lr = 1e-4\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "Enc_optimizer=optim.RMSprop(lr=lr, params=model.encoder.parameters()) # Optimiseur\n",
    "En_scheduler = torch.optim.lr_scheduler.ExponentialLR(Enc_optimizer, gamma=0.1)\n",
    "\n",
    "# Permet de changer le learning rate au cours de l'apprentissage\n",
    "# Plus on se rapproche du minimun plus le learning devient faible afin d'éviter les rebond et d'assurer la convergence vers le minimum\n",
    "loss_fn=nn.L1Loss() # La fonction de perte\n",
    "recons_loss=nn.SmoothL1Loss()\n",
    "# recons_loss=nn.MSELoss()\n",
    "\n",
    "history=model.fit(train_loader,Gen_optimizer,Disc_optimizer,Enc_optimizer,Gen_scheduler,En_scheduler,loss_fn,recons_loss,epochs=1)#Entraînement du modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ff182-bafc-4d8a-b8da-148cc1ddea90",
   "metadata": {},
   "source": [
    "### Visualisation des des learnings curves sur le jeu de données d'entraînement et de validation.\n",
    "N'ayant pas trop de données alors nous avons choisi le jeu de données de test comme je de données de validation lors de l'entraînement<br>\n",
    "Rappellons que notre jeu de données d'entraînement est uniquement constitué de données normales afin d'apprendre à reconstituer correctement ces derniers.<br>\n",
    "Cependata le jeu de données de test est constitué à la fois des données normales et des données anormales. Les données anormales détectées par notre auto encodeur seront celles dont l'erreur de reconstitution est la plus élevée<br>\n",
    "Si notres jeux de données de test contient par exemple 1.5 % de d'anomalies alors nous choisirons d'attribuer le label -1 aux 1.5 % de données ayant une haute erreur de reconstitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0e9c5-8452-473d-ba90-e672913e38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(history['generator_loss'])\n",
    "epoch=[i for i in range(1,n+1)]\n",
    "plt.plot(epoch,history['generator_loss'],label=\"generator_loss\",color='blue')\n",
    "plt.plot(epoch,history['discriminator_loss'],label=\"discriminator_loss\",color='green')\n",
    "plt.plot(epoch,history['encoder_loss'],label=\"encoder_loss\",color='red')\n",
    "\n",
    "plt.ylabel(\"MSELoss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cee51-3b44-4511-bd6f-6a10c17f49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "criterion=nn.SmoothL1Loss(reduction='none')\n",
    "ypred=model.predict(test_loader,0.02,criterion=criterion)\n",
    "ypred=inverse_transform_target(ypred)\n",
    "evaluation(ytest,ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfd158-0b7a-437b-8217-bcd17baeeb8a",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Combien de couche utilisé dans notre couche d'encodeur ?\n",
    "2. Quelle dimension pour la variable latente(la dimension de des donnée à la sortie des de la couche d'encodeur) ?\n",
    "3. Quelle est la bonne initialisation des poids de notre réseaux (Quelle fonction utilisée) ?\n",
    "4. Quel algo d'optimization utilisé (SGD, Adam, ...)?\n",
    "5. Quelle fonction d'activation utilisée ?\n",
    "6. Quelle pourcentage de contamination utilisé ?\n",
    "7. Comment détecter des problèmes de vanishing ou exploding gradient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088151d-31ae-4a03-93d5-418a9bdcfd8a",
   "metadata": {},
   "source": [
    "## Référence\n",
    "https://arxiv.org/pdf/1805.06725.pdf <br>\n",
    "https://github.com/samet-akcay/ganomaly/blob/master/options.py <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166fc0d-c4bd-4c53-9211-8737320b5218",
   "metadata": {},
   "source": [
    "meiller params : w_recons=40, dropout=0.2,recons_loss=SmoothL1,  tp=421, loss_fn=L1Loss <br>\n",
    "gen_loss= 1*loss_fn(z_real2,z_fake2) + 3*loss_fn(real_outputs2.to(self.device),fake_output2) + 40*recons_loss(real_data,fake_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eac01f-d15f-4f88-a693-c36a63860703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
