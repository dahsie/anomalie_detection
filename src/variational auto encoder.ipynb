{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c662b53e-e0e1-4f69-adb8-a159f7d83b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LinearLR,ReduceLROnPlateau  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer, roc_auc_score,confusion_matrix\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import RocCurveDisplay,roc_curve\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71c776-8a84-4d83-aec0-84550aea4192",
   "metadata": {},
   "source": [
    "### Dataset customisée par entraîner un modèle d'auto encodeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5aaeb23-22a9-4f8a-b657-7914e1f2e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_Dataset(Dataset):\n",
    "\n",
    "    \"\"\"Cette classe à été créée afin de créer facilement un DataLoader de pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "\n",
    "        super(AE_Dataset,self).__init__()\n",
    "        \n",
    "        #La target d'un auto encoder est lui le donnée initial car n'oublions pas que l'objectif \n",
    "        #de l'auto encodeur c'est de reconstituer la donnée initiale\n",
    "\n",
    "        self.data = np.copy(data)\n",
    "        self.targets = np.copy(data)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index],dtype=torch.float32), torch.tensor(self.targets[index],dtype=torch.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48194798-3fba-4e8e-9a4a-80fe67f2d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# batch, m_feature= 3, 4\n",
    "# input = torch.randn(batch, m_feature)\n",
    "# # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n",
    "# # as shown in the image below\n",
    "# # layer_norm = nn.LayerNorm([batch, m_feature])\n",
    "# layer_norm = nn.LayerNorm(batch)\n",
    "# output = layer_norm(input)\n",
    "\n",
    "# print(input)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc24e95-4789-45f1-b4a3-17e92c6e74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusin_matrix_display(ytest,ypred):\n",
    "    metrics.ConfusionMatrixDisplay(confusion_matrix(ytest,ypred)).plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "def precision_recall_display(ytest, ypred):\n",
    "    precision, recall, _ = precision_recall_curve(ytest, ypred)\n",
    "    PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "    plt.title(\"Precision_Recall_Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def roc_curve_disolay(ytest, ypred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(ytest, ypred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "    print(f\"roc_auc {roc_auc}\")\n",
    "    display.plot()\n",
    "    plt.title(\"ROC_Curve\")\n",
    "    plt.show()\n",
    "\n",
    "def scores(ytest,ypred):\n",
    "    df=pd.DataFrame({\"f1_score\" : [f1_score(ytest,ypred)],\n",
    "                     \"accuracy_score\" : [accuracy_score(ytest,ypred)],\n",
    "                     \"precision_score\" : [precision_score(ytest,ypred)],\n",
    "                     \"recall_score\" : [recall_score(ytest,ypred)]\n",
    "    })\n",
    "    print(df)\n",
    "    return df\n",
    " \n",
    "def evaluation(ytrue,ypred):\n",
    "    confusin_matrix_display(ytrue,ypred)\n",
    "    precision_recall_display(ytrue, ypred)\n",
    "    roc_curve_disolay(ytrue, ypred)\n",
    "    scores(ytest,ypred)\n",
    "\n",
    "def transform_target(target):\n",
    "    \"\"\"Args:\n",
    "        target: np.ndarry or pd.DataFrame\n",
    "    \"\"\"\n",
    "    n=target.shape[0]\n",
    "    if isinstance(target,np.ndarray):\n",
    "        target[target==1]=-1\n",
    "        target[target==0]=1\n",
    "        \n",
    "    elif isinstance(target,pd.DataFrame) or isinstance(target,pd.Series):\n",
    "        target[target==1]=-1\n",
    "        target[target==0]=1\n",
    "    return target\n",
    "\n",
    "\n",
    "def inverse_transform_target(target):\n",
    "    n=target.shape[0]\n",
    "    if isinstance(target,pd.DataFrame) or isinstance(target,pd.Series):\n",
    "        target[target==1]=0\n",
    "        target[target==-1]=1\n",
    "        \n",
    "    elif isinstance(target,np.ndarray): \n",
    "        target[target==1]=0\n",
    "        target[target==-1]=1\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975c9f6e-2d7c-4386-8706-042b5acf2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "\n",
    "    def __init__(self, file_path, scaler,test_size=0.15, random_state=42):\n",
    "        \n",
    "        self.file_path = file_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        self.data = None\n",
    "        self.target = None\n",
    "\n",
    "        # Chargement et prétraitement de la donnée\n",
    "        self._load_data()\n",
    "        self._preprocess_data()\n",
    "        self._split_data()\n",
    "\n",
    "        self.xtrain = self._xtrain # ce sont uniquement les données normales quui seront entraînées avec l'auto encoder\n",
    "        self.ytrain = self.xtrain # La target d'un auto encoder est lui le donnée initial car n'oublions pas que l'objectif \n",
    "                                # de l'auto encodeur c'est de reconstituer la donnée initiale\n",
    "\n",
    "        # Par contre les données de test devront à la fois contenir les données normales et les données anormales\n",
    "        #Ainsi les données de test seront la concatenation entre les données normales reservées pour le test et les données contenant que des 1\n",
    "\n",
    "        self.xtest = pd.concat([self._xtest, self.data[self.target==1]], axis=0)\n",
    "        self.ytest = pd.concat([self._ytest, self.target[self.target==1]], axis=0)\n",
    "\n",
    "        self.xtest.reset_index(drop=True, inplace=True)\n",
    "        self.ytest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Note : ytrain utilisé comme target lors de l'entraînement n'est le même format que ytest utilisé lors de la prédiction\n",
    "        # ytrain =xtrain alors que ytest =target (labels)(0 pour donnée normale et 1 pour donnée anormale)\n",
    "   \n",
    "        \n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "        \n",
    "    def _load_data(self):\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        \n",
    "        self.data.drop_duplicates(keep=\"last\", inplace=True)\n",
    "        self.target = self.data[\"Class\"]\n",
    "        self.data.drop([\"Class\", \"Time\"], axis=1,inplace=True)\n",
    "        \n",
    "\n",
    "    def _preprocess_data(self):\n",
    "\n",
    "        # Avec un modèle de réseaux de neurones profond, l'on n'est pas obligé de pré-traiter la donnée\n",
    "        if self.scaler is not None:\n",
    "            \n",
    "            data_scaled_array = self.scaler.fit_transform(self.data)\n",
    "            # self.data['Amount']=self.scaler.fit_transform(self.data['Amount'].values.reshape(self.data.shape[0],-1))\n",
    "\n",
    "            #Il est impératif d'ajouter l'argument index lors de la créaction d'un dataframe à partir d'un numpy array\n",
    "            # Dans notre cas, data_scaled_array est un tableau numpy. si l'argument \"index\" n'est pas ajouté alors l'index par défaut sera\n",
    "            # RangeIndex. Le RangeIndex est incompatible avec la Index. Par exemple il sera impossible de concater deux dataframes \n",
    "            # Ou un dataframe et une serie dont l'un possède Index et l'autre RangeIndex\n",
    "            self.data = pd.DataFrame(data_scaled_array, columns=self.data.columns,index=self.data.index)\n",
    "            \n",
    "\n",
    "    def _split_data(self):\n",
    "        \n",
    "        self._xtrain, self._xtest, self._ytrain, self._ytest = train_test_split(\n",
    "            self.data, self.target, test_size=self.test_size, random_state=self.random_state, shuffle=True)\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.xtrain, self.ytrain\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.xtest, self.ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c516a-6a9b-4570-88ff-3f5dc0d7be42",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3baa0fd-d461-42b5-aa3c-46e5f0646719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(nn.Module):\n",
    "\t\n",
    "    def __init__(self):\n",
    "        super(Sampling,self).__init__()\n",
    "        \n",
    "    def forward(self,mean: Tensor,log_var: Tensor) -> Tensor:\n",
    "        batch = mean.shape[0]\n",
    "        n_features = mean.shape[1]\n",
    "        epsilon = torch.randn(batch, n_features).to(mean.device)\n",
    "        return mean + torch.exp(0.5 * log_var) * epsilon # On mettre le second tensor dans le même device que le premier tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55e561-7f32-41a5-88f2-1d008b9f5ddf",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15cbce9-1d22-4703-84cd-5db8817a9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size: int, latent_size: int,dropout: float,device: str=\"cuda\") -> None:\n",
    "\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=input_size,out_features=25)\n",
    "        self.residual1=nn.Linear(in_features=input_size,out_features=20)\n",
    "        self.layer_norm1=nn.LayerNorm(25)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "        self.fc2=nn.Linear(in_features=25,out_features=20)\n",
    "        self.layer_norm2=nn.LayerNorm(20)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.residual2=nn.Linear(in_features=20,out_features=latent_size)\n",
    "        self.fc3=nn.Linear(in_features=20,out_features=15)\n",
    "        self.layer_norm3=nn.LayerNorm(15)\n",
    "        self.relu3=nn.ReLU()\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "        \n",
    "            \n",
    "        self.mean = nn.Linear(in_features=15,out_features=latent_size)\n",
    "        self.log_var = nn.Linear(in_features=15,out_features=latent_size)\n",
    "        \n",
    "        self.device= torch.device(\"cuda\") if device==\"cuda\" else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        x=self.relu1(self.layer_norm1(self.fc1(inputs)))\n",
    "        residual1=self.residual1(inputs)\n",
    "        x=self.dropout1(x)\n",
    "        x= self.relu2(self.layer_norm2(self.fc2(x)+residual1))\n",
    "        residual2=self.residual2(x)\n",
    "        x= self.relu3(self.layer_norm3(self.fc3(x)))\n",
    "        x=self.dropout2(x)\n",
    "\n",
    "        mean=self.mean(x)\n",
    "        log_var=self.log_var(x)\n",
    "        z=Sampling()(mean, log_var)\n",
    "\n",
    "        return mean,log_var,z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f9ea2-841a-432e-b478-48cadadaabf0",
   "metadata": {},
   "source": [
    "## Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab96b06-48b5-4d07-bd10-697c99e889f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,latent_size: int, output_size: int,dropout: float,device: str=\"cuda\") -> None:\n",
    "\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=latent_size,out_features=15)\n",
    "        self.residual1=nn.Linear(in_features=latent_size,out_features=20)\n",
    "        self.relu1=nn.LeakyReLU(0.1)\n",
    "        self.dropout1=nn.Dropout(dropout)\n",
    "        self.fc2=nn.Linear(in_features=15,out_features=20)\n",
    "        self.layer_norm3=nn.LayerNorm(20)\n",
    "        self.relu2=nn.LeakyReLU(0.1)\n",
    "        self.residual2=nn.Linear(in_features=20,out_features=29)\n",
    "        self.fc3=nn.Linear(in_features=20,out_features=25)\n",
    "        self.layer_norm4=nn.LayerNorm(25)\n",
    "        self.relu3=nn.LeakyReLU(0.1)\n",
    "        self.dropout2=nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc4=nn.Linear(in_features=25,out_features=output_size)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        x=self.relu1(self.fc1(inputs))\n",
    "        residual1=self.residual1(inputs)\n",
    "        x=self.dropout1(x)\n",
    "        x= self.relu2(self.layer_norm3(self.fc2(x) +residual1))\n",
    "        residual2=self.residual2(x)\n",
    "        x= self.relu3(self.layer_norm4(self.fc3(x)))\n",
    "        x=self.dropout2(x)\n",
    "        x=self=self.fc4(x) +residual2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c14f8f-0a44-4d7a-8097-e128fc3bfe3f",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a353d927-e332-4123-94c2-dda7fcbfc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size: int,latent_size: int,output_size: int,contamination: float=0.5,dropout: float=0.1,device: str=\"cuda\") -> None:\n",
    "\n",
    "        super(VariationalAutoEncoder,self).__init__()\n",
    "        \n",
    "        self.encoder=Encoder(input_size=input_size, latent_size = latent_size,device=\"cuda\",dropout=0.1)\n",
    "        self.latent=None\n",
    "        self.mean=None\n",
    "        self.log_var=None\n",
    "        self.decoder=Decoder(latent_size =latent_size, output_size=output_size,device=\"cuda\",dropout=0.1)\n",
    "        self.offset=contamination\n",
    "        \n",
    "        self.device= torch.device(\"cuda\") if device==\"cuda\" else torch.device(\"cpu\")\n",
    "\n",
    "        self.is_fitted=False\n",
    "\n",
    "        self.total_train_curve = []\n",
    "        self.recons_train_curve = []\n",
    "        self.kld_train_curve = []\n",
    "\n",
    "        self.total_val_curve = []\n",
    "        self.recons_val_curve = []\n",
    "        self.kld_val_curve = []\n",
    "\n",
    "    def forward(self,inputs: Tensor) -> Tensor:\n",
    "        mean,log_var,z=self.encoder(inputs)\n",
    "        self.latent=z\n",
    "        self.mean=mean\n",
    "        self.log_var=log_var\n",
    "        output=self.decoder(z)\n",
    "\n",
    "        return mean,log_var,output\n",
    "\n",
    "    \n",
    "    def train_one_epoch(self,train_loader:DataLoader,optimizer,scheduler):\n",
    "        \n",
    "        \"\"\"The function is use to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer      : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function  : The loss function to use during the model training. CrossEntropyLoss and MSELoss are very often use for respectively\n",
    "                                classification and regression problem\n",
    "                train_loader   : A torch DataLoader which is use to iterate through the data during train step\n",
    "\n",
    "            Return : The last loss value on the train_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        running_loss = 0.0\n",
    "        running_kld_loss=0.0\n",
    "        running_recons_loss=0.0\n",
    "        \n",
    "        last_loss = {\n",
    "            \"kld_loss\":0.0,\n",
    "            \"recons_loss\":0.0,\n",
    "            \"total_loss\":0.0\n",
    "        }\n",
    "        \n",
    "        for batch_index, batch in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad() # Putting gradient to zero in order to accumulate upcoming gradient\n",
    "\n",
    "            \n",
    "            data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "          \n",
    "            mean,log_var,outputs=self.forward(data) # Making prediction\n",
    "\n",
    "            #Reconstition Loss\n",
    "            recon_loss, kld_loss=self.loss_function(obs=target, recon=outputs, mu=mean, log_var=log_var)\n",
    "\n",
    "            \n",
    "            tolal_loss=recon_loss + kld_loss\n",
    "            tolal_loss.backward() # Computing the gradient\n",
    "\n",
    "            running_kld_loss+=kld_loss.item()\n",
    "            running_recons_loss+=recon_loss.item()\n",
    "            running_loss+=tolal_loss.item() # Accumulation loss through one epoch\n",
    "\n",
    "            optimizer.step() # Updating weights\n",
    "\n",
    "            if batch_index%100==99:\n",
    "                last_loss[\"total_loss\"] = running_loss/100 # Computing loss per batch\n",
    "                last_loss[\"kld_loss\"] = running_kld_loss/100\n",
    "                last_loss[\"recons_loss\"] = running_kld_loss/100\n",
    "                # print(\"batch {} loss {}\".format(batch_index+1,last_loss))\n",
    "        \n",
    "        before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        scheduler.step()\n",
    "        after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        # print(\"SGD lr %.8f -> %.8f\" % (before_lr, after_lr))\n",
    "        return last_loss\n",
    "\n",
    "\n",
    "    def validation_one_epoch(self,validation_loader):\n",
    "        \n",
    "        \"\"\"The function is used to train one epoch of the model\n",
    "            Args: \n",
    "                optimizer       : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                loss_function   : The loss function to use during the model validation. Same as the training one\n",
    "                val_loader      : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : The last loss value on the val_loader\n",
    "                \n",
    "        \"\"\"\n",
    "        avg_vloss = {\n",
    "            \"kld_loss\":0.0,\n",
    "            \"recons_loss\":0.0,\n",
    "            \"total_loss\":0.0\n",
    "        }\n",
    "        \n",
    "        running_loss=0.\n",
    "        running_kld_loss=0.\n",
    "        running_recons_loss=0.\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "            for batch in validation_loader:\n",
    "    \n",
    "                data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "                mean,log_var,outputs=self.forward(data) # Making prediction\n",
    "\n",
    "                #Reconstition Loss\n",
    "                recon_loss, kld_loss=self.loss_function(obs=target, recon=outputs, mu=mean, log_var=log_var)\n",
    "    \n",
    "                tolal_loss=recon_loss + kld_loss\n",
    "\n",
    "                running_kld_loss+=kld_loss.item()\n",
    "                running_recons_loss+=recon_loss.item()\n",
    "                running_loss+=tolal_loss.item() \n",
    "\n",
    "\n",
    "        avg_vloss[\"total_loss\"] = running_loss/len(validation_loader) # Computing loss per batch\n",
    "        avg_vloss[\"kld_loss\"] = running_kld_loss/len(validation_loader)\n",
    "        avg_vloss[\"recons_loss\"] = running_recons_loss/len(validation_loader)\n",
    "        \n",
    "        return avg_vloss\n",
    "\n",
    "\n",
    "    def loss_function(self, obs: Tensor, recon: Tensor, mu: Tensor, log_var: Tensor):\n",
    "        \n",
    "        recon_loss = nn.MSELoss(reduction='mean')(recon, obs)\n",
    "    \n",
    "        kld_loss = -0.5 * torch.mean(1 + log_var - mu**2 - log_var.exp())\n",
    "       \n",
    "        return recon_loss, kld_loss\n",
    "\n",
    "\n",
    "    def predict(self, dataloader,loss_fn=nn.MSELoss(reduction='none')):\n",
    "        assert self.is_fitted==True, \"Le modèle n'est pas encore entraîné donc ne peut pas faire de prédiction\"\n",
    "        return self.decision_function(dataloader,loss_fn)\n",
    "\n",
    "\n",
    "    def decision_function(self,dataloader: DataLoader,loss_fn):\n",
    "        \n",
    "        dataframe=self.compute_score_sample(dataloader,loss_fn)\n",
    "        dataframe.loc[-dataframe[\"score_sample\"] < (-dataframe[\"score_sample\"]).quantile(self.offset), \"label\"] = -1\n",
    "        \n",
    "        return dataframe['label']\n",
    "\n",
    "    def score_sample(self,dataloader: DataLoader,loss_fn):\n",
    "        \n",
    "        dataframe=self.compute_score_sample(self,dataloader,loss_fn)\n",
    "\n",
    "        return dataframe[\"score_sample\"]\n",
    "        \n",
    "    def compute_score_sample(self,dataloader: DataLoader,loss_fn):\n",
    "\n",
    "        assert loss_fn.reduction=='none',\"La fonction pour le calcul des erreurs de reconstitution ne possède pas les bonne argument\"\n",
    "        \n",
    "        reconstition_errors=[]\n",
    "        with torch.no_grad() :\n",
    "            for batch in dataloader:\n",
    "    \n",
    "                data,target=batch[0].to(self.device),batch[1].to(self.device)\n",
    "                mean,log_var,outputs=self.forward(data) # Making prediction\n",
    "\n",
    "                loss=loss_fn(outputs,target) # Computing the loss\n",
    "\n",
    "                for item in loss:\n",
    "                    reconstition_errors.append(torch.sum(item).item())\n",
    "    \n",
    "        \n",
    "        dataframe=pd.DataFrame({\n",
    "            \"score_sample\" : reconstition_errors,\n",
    "            \"label\" :[1 for _ in range(len(reconstition_errors))]\n",
    "        })\n",
    "\n",
    "        return dataframe\n",
    "                \n",
    "\n",
    "    def fit(self,train_loader,validation_loader,optimizer,scheduler,epochs=100,device=\"cuda\"):\n",
    "        \"\"\"The function is used to train the model through many epochs\n",
    "            Args: \n",
    "                optimizer             : The optimizer algorithm to be use. SDG or Adam are used in most of cases\n",
    "                train_loader          : A torch DataLoader which is use to iterate through the data during train step\n",
    "                loss_function         : The loss function to use during the model validation. Same as the training one\n",
    "                validataion_loader    : A torch DataLoader which is use to iterate through the data during validation step\n",
    "\n",
    "            Return : No return\n",
    "                \n",
    "        \"\"\"\n",
    "        self.to(self.device)\n",
    "        best_avg_vloss=1_000_000_000\n",
    "        for epoch in range(epochs):\n",
    "            # print(\"\\n\")\n",
    "            print(\"EPOCHS : {}\".format(epoch +1))\n",
    "            self.train(True) # Activate training mode\n",
    "            avg_loss=self.train_one_epoch(train_loader,optimizer,scheduler) # training the model on an epoch\n",
    "            \n",
    "            self.eval() # Setting the evaluation mode so that no gradient will be compute, that will save forward pass time\n",
    "            avg_vloss=self.validation_one_epoch(validation_loader) # Evalution the model after one epoch\n",
    "            # if epoch%10==0:\n",
    "            print(f\"total_Loss {avg_loss['total_loss']} vs total_Validation_loss  {avg_vloss['total_loss']}\")\n",
    "            # print(f\"recons_Loss {avg_loss['recons_loss']} vs recons_Validation_loss  {avg_vloss['recons_loss']}\")\n",
    "            # print(f\"kld_loss_Loss {avg_loss['kld_loss']} vs kld_loss_Validation_loss  {avg_vloss['kld_loss']}\")\n",
    "            \n",
    "            self.total_train_curve.append(avg_loss['total_loss'])\n",
    "            self.recons_train_curve.append(avg_loss['recons_loss'])\n",
    "            self.kld_train_curve.append(avg_loss['kld_loss'])\n",
    "            self.total_val_curve.append(avg_vloss['total_loss'])\n",
    "            self.recons_val_curve.append(avg_vloss['recons_loss'])\n",
    "            self.kld_val_curve.append(avg_vloss['kld_loss'])\n",
    "            \n",
    "\n",
    "            if avg_loss['total_loss'] < best_avg_vloss :\n",
    "                best_avg_vloss=avg_loss['total_loss']\n",
    "                model_path='/home/dah/timeSeries/time_series_forcasting/models/model_{}'.format(datetime.now().strftime('%Y%m%d_%H%')) \n",
    "                torch.save(self.cpu().state_dict(),model_path)\n",
    "                self.to(self.device)\n",
    "                self.to(self.device)\n",
    "        self.is_fitted=True\n",
    "        \n",
    "        return {\n",
    "            \"total_train_curve\" : self.total_train_curve,\n",
    "            \"recons_train_curve\" : self.recons_train_curve,\n",
    "            \"kld_train_curve\": self.kld_train_curve,\n",
    "            \"total_val_curve\" : self.total_val_curve,\n",
    "            \"recons_val_curve\" : self.recons_val_curve,\n",
    "            \"kld_val_curve\": self.kld_val_curve\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c20c6c-762f-4ce2-804e-fd2bf8ebba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCHS : 1\n",
      "total_Loss 1.6264096868038178 vs total_Validation_loss  0.011023786748788877\n",
      "EPOCHS : 2\n",
      "total_Loss 0.4494631083868444 vs total_Validation_loss  0.006265023828353854\n",
      "EPOCHS : 3\n",
      "total_Loss 0.29133600239641966 vs total_Validation_loss  0.004700218094566917\n",
      "EPOCHS : 4\n",
      "total_Loss 0.21321352063678206 vs total_Validation_loss  0.003915779988370579\n",
      "EPOCHS : 5\n",
      "total_Loss 0.16710023342631758 vs total_Validation_loss  0.003457186207576433\n",
      "EPOCHS : 6\n",
      "total_Loss 0.13749752091709525 vs total_Validation_loss  0.003160438526013466\n",
      "EPOCHS : 7\n",
      "total_Loss 0.11758806613041088 vs total_Validation_loss  0.002951015360623323\n",
      "EPOCHS : 8\n",
      "total_Loss 0.10376109110657125 vs total_Validation_loss  0.002799061644195058\n",
      "EPOCHS : 9\n",
      "total_Loss 0.09371266988571733 vs total_Validation_loss  0.0026805253140228026\n",
      "EPOCHS : 10\n",
      "total_Loss 0.08624407096533104 vs total_Validation_loss  0.0025931120222141414\n",
      "EPOCHS : 11\n",
      "total_Loss 0.0804186148243025 vs total_Validation_loss  0.0025244100029289474\n",
      "EPOCHS : 12\n",
      "total_Loss 0.07592983373207972 vs total_Validation_loss  0.0024641080069520986\n",
      "EPOCHS : 13\n",
      "total_Loss 0.07226088755298406 vs total_Validation_loss  0.0024144815888643596\n",
      "EPOCHS : 14\n",
      "total_Loss 0.06921969297807663 vs total_Validation_loss  0.0023761295770447733\n",
      "EPOCHS : 15\n",
      "total_Loss 0.06683388677891344 vs total_Validation_loss  0.0023376340863704302\n",
      "EPOCHS : 16\n",
      "total_Loss 0.06469371061539278 vs total_Validation_loss  0.0023128484412776756\n",
      "EPOCHS : 17\n",
      "total_Loss 0.06292367598973214 vs total_Validation_loss  0.0022892228663738013\n",
      "EPOCHS : 18\n",
      "total_Loss 0.061386529931332916 vs total_Validation_loss  0.002264751658821868\n",
      "EPOCHS : 19\n",
      "total_Loss 0.06006893011974171 vs total_Validation_loss  0.0022436766037260887\n",
      "EPOCHS : 20\n",
      "total_Loss 0.0589819510653615 vs total_Validation_loss  0.0022290221811283022\n",
      "EPOCHS : 21\n",
      "total_Loss 0.057862861297326165 vs total_Validation_loss  0.0022137876556263146\n",
      "EPOCHS : 22\n",
      "total_Loss 0.056904155258089305 vs total_Validation_loss  0.0021959103307658398\n",
      "EPOCHS : 23\n",
      "total_Loss 0.05615883773192763 vs total_Validation_loss  0.0021830582714810116\n",
      "EPOCHS : 24\n",
      "total_Loss 0.05540553159662522 vs total_Validation_loss  0.002176017120027643\n",
      "EPOCHS : 25\n",
      "total_Loss 0.054792399187572303 vs total_Validation_loss  0.0021608774251106358\n",
      "EPOCHS : 26\n",
      "total_Loss 0.05413794315652922 vs total_Validation_loss  0.0021513734565652668\n",
      "EPOCHS : 27\n",
      "total_Loss 0.0535523987992201 vs total_Validation_loss  0.002142892067656475\n",
      "EPOCHS : 28\n",
      "total_Loss 0.05310181269189343 vs total_Validation_loss  0.002133907783937404\n",
      "EPOCHS : 29\n",
      "total_Loss 0.052692222694167866 vs total_Validation_loss  0.0021249464668461632\n",
      "EPOCHS : 30\n",
      "total_Loss 0.05223269998794421 vs total_Validation_loss  0.002120155116789204\n",
      "EPOCHS : 31\n",
      "total_Loss 0.051819616907741874 vs total_Validation_loss  0.0021117134013511397\n",
      "EPOCHS : 32\n",
      "total_Loss 0.05148402203456499 vs total_Validation_loss  0.0021062781249610515\n",
      "EPOCHS : 33\n",
      "total_Loss 0.05115347187384032 vs total_Validation_loss  0.002101065702564892\n",
      "EPOCHS : 34\n"
     ]
    }
   ],
   "source": [
    "# Reprodductibilité\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Creation Dataset \n",
    "dataset = CustomDataset(file_path=\"/home/dah/anomalie_detection/anomalie_detection/data/creditcard.csv\",scaler=MinMaxScaler())\n",
    "\n",
    "xtrain, ytrain = dataset.get_train_data()\n",
    "xtest, ytest = dataset.get_test_data()\n",
    "\n",
    "\n",
    "#Creation de DataLoader pytorch (Just un objet qui permet d'itérer plus faciler sur l'entièreté de la dataset)\n",
    "pytorch_train_dataset = AE_Dataset(xtrain)\n",
    "pytorch_test_dataset = AE_Dataset(xtest)\n",
    "train_loader=DataLoader(dataset=pytorch_train_dataset,batch_size=100,num_workers=14)\n",
    "test_loader=DataLoader(dataset=pytorch_test_dataset,batch_size=10,num_workers=14)\n",
    "\n",
    "#Creation de modeèle\n",
    "model=VariationalAutoEncoder(input_size=29,latent_size=10,output_size=29,contamination=0.02,dropout=0.3,device=\"cuda\")\n",
    "\n",
    "#Creation des paramètres d'entrînement du modèle\n",
    "optimizer=optim.SGD(lr=1e-2, params=model.parameters(),momentum=0.9) # Optimiseur\n",
    "scheduler = LinearLR(optimizer, start_factor=1e-1, end_factor=1e-4, total_iters=100) # Permet de changer le learning rate au cours de l'apprentissage\n",
    "\n",
    "                                                                                    #Plus on se rapproche du minimun plus le learning devient faible afin d'éviter les rebond et d'assurer la convergence vers le minimum\n",
    "\n",
    "\n",
    "history=model.fit(train_loader,test_loader,optimizer,scheduler,epochs=100)#Entraînement du modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ff182-bafc-4d8a-b8da-148cc1ddea90",
   "metadata": {},
   "source": [
    "### Visualisation des des learnings curves sur le jeu de données d'entraînement et de validation.\n",
    "N'ayant pas trop de données alors nous avons choisi le jeu de données de test comme je de données de validation lors de l'entraînement<br>\n",
    "Rappellons que notre jeu de données d'entraînement est uniquement constitué de données normales afin d'apprendre à reconstituer correctement ces derniers.<br>\n",
    "Cependata le jeu de données de test est constitué à la fois des données normales et des données anormales. Les données anormales détectées par notre auto encodeur seront celles dont l'erreur de reconstitution est la plus élevée<br>\n",
    "Si notres jeux de données de test contient par exemple 1.5 % de d'anomalies alors nous choisirons d'attribuer le label -1 aux 1.5 % de données ayant une haute erreur de reconstitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0e9c5-8452-473d-ba90-e672913e38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(history['total_train_curve'])\n",
    "epoch=[i for i in range(2,n+1)]\n",
    "plt.plot(epoch,history['total_train_curve'][1:],label=\"Training curve\")\n",
    "plt.plot(epoch,history['total_val_curve'][1:],label=\"Validation curve\")\n",
    "\n",
    "plt.ylabel(\"MSELoss + KLDLoss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae6b488-eaa4-4ff5-bdae-112c54888371",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(history['total_train_curve'])\n",
    "epoch=[i for i in range(2,n+1)]\n",
    "plt.plot(epoch,history['recons_train_curve'][1:],label=\"Training curve\")\n",
    "plt.plot(epoch,history['recons_val_curve'][1:],label=\"Validation curve\")\n",
    "\n",
    "plt.ylabel(\"MSELoss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2c9f7-40bc-41a5-8866-fb2f481bdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(history['total_train_curve'])\n",
    "epoch=[i for i in range(2,n+1)]\n",
    "plt.plot(epoch,history['kld_train_curve'][1:],label=\"Training curve\")\n",
    "plt.plot(epoch,history['kld_val_curve'][1:],label=\"Validation curve\")\n",
    "\n",
    "plt.ylabel(\"KLDLoss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cee51-3b44-4511-bd6f-6a10c17f49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.MSELoss(reduction='none') # Fonction de calcul de score d'anomalie\n",
    "ypred=model.predict(test_loader,loss_fn=criterion)\n",
    "ypred=inverse_transform_target(ypred)\n",
    "evaluation(ytest,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f41fa-6972-4068-b054-6d5a93ba2f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
